library(EBImage)
if(!require("EBImage")) install.packages("EBImage"); library("EBImage")
install.packages("png")
library(png)
install.packages("magick")
tiger <- image_read_svg('chase.png', width = 350)
library(magick)
tiger <- image_read_svg('chase.png', width = 350)
tiger <- image_read_png('chase.png', width = 350)
tiger <- image_read('chase.png', width = 350)
tiger <- image_read('chase.png')
print(tiger)
runApp()
print(tiger)
runApp()
runApp()
runApp()
runApp()
# 1. Remove punctuation and numbers with regular expressions
chase_comments <- mutate(chase_nov19_jan20, message = gsub(x = text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = ""))
# 4. Correct Spelling
correct_spelling <- function(input) {
output <- case_when(
# any manual corrections
input == 'license' ~ 'licence',
# check and (if required) correct spelling
!hunspell_check(input, dictionary('en_GB')) ~
hunspell_suggest(input, dictionary('en_GB')) %>%
# get first suggestion, or NA if suggestions list is empty
map(1, .default = NA) %>%
unlist(),
TRUE ~ input # if word is correct
)
# if input incorrectly spelled but no suggestions, return input word
ifelse(is.na(output), input, output)
}
# 7. Create the document-term matrix
#chase_tokenized <- chase_tokenized %>% count(user_id,word)
#save(chase_tokenized,file="chase_tokenized.Rda")
load("chase_tokenized.Rda")
chase_tokenized <- chase_tokenized %>% count(user_id,word)
ChaseDTM <- chase_tokenized %>% cast_dtm(user_id,word,n,weighting = tm::weightTfIdf)
findAssocs(ChaseDTM, terms = "Chase", corlimit = 0.1)
Chasefreq <- chase_tokenized %>% group_by(word) %>% # for this, we need to have the sum over all documents
summarize(freq = n()) %>%
arrange(-freq)                  # arrange = order; from most frequent term to lowest frequent
head(Chasefreq)
tf <- termFreq(chase_comments$message)
wordcloud(names(tf),tf,
max.words=40,
scale=c(0.7,0.7))
wordcloud(Chasefreq$word, Chasefreq$freq,
max.words=40,
scale=c(0.9,0.9))
get_sentiments("bing") %>%
count(sentiment)
chase_sentiment <- inner_join(chase_tokenized,get_sentiments("bing"))
#Sentiment
summary_sentiment <- chase_sentiment %>%  count(word,sentiment,sort=TRUE) %>%
group_by(sentiment) %>%
top_n(10) %>%
arrange(n) %>%
as.data.frame(stringsAsFactors=FALSE)
par(oma=c(0,4,0,0),mfrow=c(1,2))
barplot(summary_sentiment[summary_sentiment$sentiment == "negative",3],names.arg = summary_sentiment[summary_sentiment$sentiment == "negative",1],horiz=TRUE,las=1,xlim=c(0,15),col="red",axes=TRUE)
barplot(summary_sentiment[summary_sentiment$sentiment == "positive",3],names.arg = summary_sentiment[summary_sentiment$sentiment == "positive",1],horiz=TRUE,las=1,xlim=c(0,15),col="green")
save(summary_sentiment,file= 'sentimentwhole.rda')
runApp()
#Summary per Post
status_sentiment <- chase_sentiment %>%
count(user_id, sentiment) %>%                # count the positives and negatives per id (status)
spread(sentiment, n, fill = 0) %>%      # we want overall sentiment, and here we have text (positive and negative), making it difficult to direclty calculate sentiment.
mutate(sentiment = positive - negative)
status_sentiment <- inner_join(chase_tokenized,get_sentiments("afinn")) %>%
group_by(user_id) %>%                      # here we get numeric values, so we can just sum them per post
summarize(Sentiment = sum(value))
mean(status_sentiment$Sentiment)
runApp()
runApp()
runApp()
runApp()
plt <- word.count %>%
# Set count threshold.
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
theme_light() +
geom_col(fill = 'black', alpha = 0.8) +
xlab(NULL) +
coord_flip() +
ggtitle(label = 'Top Word Count')
plt %>% ggplotly()
load("chase_nov19_jan20.Rda")
# 1. Remove punctuation and numbers with regular expressions
chase_comments <- mutate(chase_nov19_jan20, message = gsub(x = text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = ""))
tweets.df <- chase_comments %>%
select(text) %>%
mutate(text= text %>% str_remove_all(pattern = '\\n')) %>%
mutate(text = text %>% str_remove_all(pattern = '&amp')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http')) %>%
# Remove hashtags.
mutate(text = text %>% str_remove_all(pattern = '#[a-z,A-Z]*')) %>%
# Remove accounts.
mutate(text = text %>% str_remove_all(pattern = '@[a-z,A-Z]*')) %>%
# Remove retweets.
mutate(text = text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: ')) %>%
mutate(text = text %>% str_remove(pattern = '^(rt)')) %>%
mutate(text = text %>% str_remove_all(pattern = '\\_'))
# Replace accents.
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
tweets.df %<>%
mutate(text = chartr(old = names(replacement.list) %>% str_c(collapse = ''),
new = replacement.list %>% str_c(collapse = ''),
x = text))
corpus <-  Corpus(x = VectorSource(x = tweets.df$text))
tweets.text <- corpus %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords('english')) %>%
tm_map(PlainTextDocument) # %>%
# Recover data into original tibble.
tweets.df %<>% mutate(text = tweets.text[[1]]$content)
#function for hashtags
GetHashtags <- function(tweet) {
hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>%
as.character()
hashtag.string <- NA
if (length(hashtag.vector) > 0) {
hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
}
return(hashtag.string)
}
hashtags.df <- tibble(
Hashtags = chase_comments$text %>% map_chr(.f = ~ GetHashtags(tweet = .x))
)
hashtags.df %>% head()
tweets.df %<>% bind_cols(hashtags.df)
stopwords.df <- tibble(
word = c(stopwords(kind = 'en'))
)
words.df <- tweets.df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(y = stopwords.df, by = 'word')
word.count <- words.df %>% count(word, sort = TRUE)
word.count %>% head(10)
plt <- word.count %>%
# Set count threshold.
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
theme_light() +
geom_col(fill = 'black', alpha = 0.8) +
xlab(NULL) +
coord_flip() +
ggtitle(label = 'Top Word Count')
plt %>% ggplotly()
#wordcloud
wordcloud(
words = word.count$word,
freq = word.count$n,
min.freq = 100,
colors = brewer.pal(8, 'Dark2')
)
hashtags.unnested.df <- tweets.df %>%
select( Hashtags) %>%
unnest_tokens(input = Hashtags, output = hashtag)
hashtags.unnested.count <- hashtags.unnested.df %>%
count(hashtag) %>%
drop_na()
wordcloud(
words = str_c('#',hashtags.unnested.count$hashtag),
freq = hashtags.unnested.count$n,
min.freq = 5,
colors=brewer.pal(8, 'Dark2')
)
wordcloud2(hashtags.unnested.count,
size = 1.5,
color = "skyblue",
backgroundColor = "whitesmoke")
bi.gram.words <- tweets.df %>%
unnest_tokens(
input = text,
output = bigram,
token = 'ngrams',
n = 2
) %>%
filter(! is.na(bigram))
bi.gram.words %>%
select(bigram) %>%
head(10)
bi.gram.words %<>%
separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>%
filter(! word1 %in% stopwords.df$word) %>%
filter(! word2 %in% stopwords.df$word) %>%
filter(! is.na(word1)) %>%
filter(! is.na(word2))
bi.gram.count <- bi.gram.words %>%
count(word1, word2, sort = TRUE) %>%
# We rename the weight column so that the
# associated network gets the weights (see below).
rename(weight = n)
bi.gram.count %>% head()
bi.gram.count %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram() +
labs(title = "Bigram Weight Distribution")
bi.gram.count %>%
mutate(weight = log(weight + 1)) %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram( binwidth = NULL) +
labs(title = "Bigram log-Weight Distribution")
threshold <- 30
# For visualization purposes we scale by a global factor.
ScaleWeight <- function(x, lambda) {
x / lambda
}
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>%
graph_from_data_frame(directed = FALSE)
# verify we have a weighted network:
is.weighted(network)
plot(
network,
vertex.size = 1,
vertex.label.color = 'black',
vertex.label.cex = 0.7,
vertex.label.dist = 1,
edge.color = 'gray',
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
save(bi.gram.count,file = "bigramcount.rda")
runApp()
bi.gram.count %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram() +
labs(title = "Bigram Weight Distribution")
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
#Note that curve is very skewed, for visualization purposes it might be a good idea to perform a transformation, e.g. log transform:
bi.gram.count %>%
mutate(weight = log(weight + 1)) %>%
ggplot(mapping = aes(x = weight)) +
theme_light() +
geom_histogram( binwidth = NULL) +
labs(title = "Bigram log-Weight Distribution")
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
#For visualization purposes, we can set a threshold which defines the minimal weight allowed in the graph.
#Remark: It is necessary to set the weight column name as weight (see igraph docs).
threshold <- 30
# For visualization purposes we scale by a global factor.
ScaleWeight <- function(x, lambda) {
x / lambda
}
network <-  bi.gram.count %>%
filter(weight > threshold) %>%
mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>%
graph_from_data_frame(directed = FALSE)
# verify we have a weighted network:
is.weighted(network)
#Visualization :To visualize the network (here is a great reference for it) we can simply use the plot function with some additional parameters:
plot(
network,
vertex.size = 1,
vertex.label.color = 'black',
vertex.label.cex = 0.7,
vertex.label.dist = 1,
edge.color = 'gray',
main = 'Bigram Count Network',
sub = glue('Weight Threshold: {threshold}'),
alpha = 50
)
runApp()
runApp()
tweetsLDA_df <- chase_comments %>%
select(text,status_id) %>%
mutate(text= text %>% str_remove_all(pattern = '\\n')) %>%
mutate(text = text %>% str_remove_all(pattern = '&amp')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http')) %>%
# Remove hashtags.
mutate(text = text %>% str_remove_all(pattern = '#[a-z,A-Z]*')) %>%
# Remove accounts.
mutate(text = text %>% str_remove_all(pattern = '@[a-z,A-Z]*')) %>%
# Remove retweets.
mutate(text = text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: ')) %>%
mutate(text = text %>% str_remove(pattern = '^(rt)')) %>%
mutate(text = text %>% str_remove_all(pattern = '\\_'))
# Replace accents.
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
tweetsLDA_df %<>%
mutate(text = chartr(old = names(replacement.list) %>% str_c(collapse = ''),
new = replacement.list %>% str_c(collapse = ''),
x = text))
tweetsTokenized <- tweetsLDA_df %>% unnest_tokens(output = "word",
input = text,
token = "words",
drop=FALSE,to_lower=TRUE)
tweetsTokenized <- tweetsTokenized %>%
anti_join(stop_words) %>%       # note that we use all stopword dictionaries here
count(status_id,word , sort=TRUE) %>%
cast_dtm(document = status_id, term = word,
value = n, weighting = tm::weightTf)
if (!require("topicmodels")) install.packages("topicmodels", quiet=TRUE) ; require("topicmodels")
tweets_lda <- LDA(tweetsTokenized, k = 3,method="gibbs",control = list(nstart = 5, burnin = 2000, best = TRUE, seed = 2:6) )
runApp()
runApp()
save(top_tweet_terms,file= "LDA_top_terms.rda")
top_tweet_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
# Replace accents.
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
tweetsLDA_df %<>%
mutate(text = chartr(old = names(replacement.list) %>% str_c(collapse = ''),
new = replacement.list %>% str_c(collapse = ''),
x = text))
tweetsTokenized <- tweetsLDA_df %>% unnest_tokens(output = "word",
input = text,
token = "words",
drop=FALSE,to_lower=TRUE)
tweetsLDA_df <- chase_comments %>%
select(text,status_id) %>%
mutate(text= text %>% str_remove_all(pattern = '\\n')) %>%
mutate(text = text %>% str_remove_all(pattern = '&amp')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http')) %>%
# Remove hashtags.
mutate(text = text %>% str_remove_all(pattern = '#[a-z,A-Z]*')) %>%
# Remove accounts.
mutate(text = text %>% str_remove_all(pattern = '@[a-z,A-Z]*')) %>%
# Remove retweets.
mutate(text = text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: ')) %>%
mutate(text = text %>% str_remove(pattern = '^(rt)')) %>%
mutate(text = text %>% str_remove_all(pattern = '\\_'))
# Replace accents.
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
tweetsLDA_df %<>%
mutate(text = chartr(old = names(replacement.list) %>% str_c(collapse = ''),
new = replacement.list %>% str_c(collapse = ''),
x = text))
tweetsTokenized <- tweetsLDA_df %>% unnest_tokens(output = "word",
input = text,
token = "words",
drop=FALSE,to_lower=TRUE)
tweetsTokenized <- tweetsTokenized %>%
anti_join(stop_words) %>%       # note that we use all stopword dictionaries here
count(status_id,word , sort=TRUE) %>%
cast_dtm(document = status_id, term = word,
value = n, weighting = tm::weightTf)
if (!require("topicmodels")) install.packages("topicmodels", quiet=TRUE) ; require("topicmodels")
tweets_lda <- LDA(tweetsTokenized, k = 3,method="gibbs",control = list(nstart = 5, burnin = 2000, best = TRUE, seed = 2:6) )
tweets_lda
tweet_topics <- tidy(tweets_lda, matrix = "beta")
# you can use the following code to get the top terms per topic
top_tweet_terms <- tweet_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_tweet_terms
save(top_tweet_terms,file= "LDA_top_terms.rda")
top_tweet_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
runApp()
runApp()
wordcloud(
words = str_c('#',hashtags.unnested.count$hashtag),
freq = hashtags.unnested.count$n,
min.freq = 5,
colors=brewer.pal(8, 'Dark2')
)
#wordcloud
wordcloud(
words = word.count$word,
freq = word.count$n,
min.freq = 100,
colors = brewer.pal(8, 'Dark2')
)
plt <- word.count %>%
# Set count threshold.
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
theme_light() +
geom_col(fill = 'black', alpha = 0.8) +
xlab(NULL) +
coord_flip() +
ggtitle(label = 'Top Word Count')
plt %>% ggplotly()
#wordcloud
wordcloud(
words = word.count$word,
freq = word.count$n,
min.freq = 100,
colors = brewer.pal(8, 'Dark2')
)
load("chase_nov19_jan20.Rda")
# 1. Remove punctuation and numbers with regular expressions
chase_comments <- mutate(chase_nov19_jan20, message = gsub(x = text, pattern = "[0-9]+|[[:punct:]]|\\(.*\\)", replacement = ""))
tweets.df <- chase_comments %>%
select(text) %>%
mutate(text= text %>% str_remove_all(pattern = '\\n')) %>%
mutate(text = text %>% str_remove_all(pattern = '&amp')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http://t.co/[a-z,A-Z,0-9]*')) %>%
mutate(text = text %>% str_remove_all(pattern = 'https')) %>%
mutate(text = text %>% str_remove_all(pattern = 'http')) %>%
# Remove hashtags.
mutate(text = text %>% str_remove_all(pattern = '#[a-z,A-Z]*')) %>%
# Remove accounts.
mutate(text = text %>% str_remove_all(pattern = '@[a-z,A-Z]*')) %>%
# Remove retweets.
mutate(text = text %>% str_remove_all(pattern = 'rt [a-z,A-Z]*: ')) %>%
mutate(text = text %>% str_remove(pattern = '^(rt)')) %>%
mutate(text = text %>% str_remove_all(pattern = '\\_'))
# Replace accents.
replacement.list <- list('á' = 'a', 'é' = 'e', 'í' = 'i', 'ó' = 'o', 'ú' = 'u')
tweets.df %<>%
mutate(text = chartr(old = names(replacement.list) %>% str_c(collapse = ''),
new = replacement.list %>% str_c(collapse = ''),
x = text))
corpus <-  Corpus(x = VectorSource(x = tweets.df$text))
tweets.text <- corpus %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords('english')) %>%
tm_map(PlainTextDocument) # %>%
# We could also use stemming by uncommenting the folowing line.
# tm_map(stemDocument, 'english')
# Recover data into original tibble.
tweets.df %<>% mutate(text = tweets.text[[1]]$content)
#function for hashtags
GetHashtags <- function(tweet) {
hashtag.vector <- str_extract_all(string = tweet, pattern = '#\\S+', simplify = TRUE) %>%
as.character()
hashtag.string <- NA
if (length(hashtag.vector) > 0) {
hashtag.string <- hashtag.vector %>% str_c(collapse = ', ')
}
return(hashtag.string)
}
#apply it to our data
hashtags.df <- tibble(
Hashtags = chase_comments$text %>% map_chr(.f = ~ GetHashtags(tweet = .x))
)
hashtags.df %>% head()
#merge these data frames together.
tweets.df %<>% bind_cols(hashtags.df)
#We begin by counting the most popular words in the tweets.
# Remove the stopwords
stopwords.df <- tibble(
word = c(stopwords(kind = 'en'))
)
words.df <- tweets.df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(y = stopwords.df, by = 'word')
word.count <- words.df %>% count(word, sort = TRUE)
word.count %>% head(10)
#visualize these counts in a bar plot
plt <- word.count %>%
# Set count threshold.
filter(n > 500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
theme_light() +
geom_col(fill = 'black', alpha = 0.8) +
xlab(NULL) +
coord_flip() +
ggtitle(label = 'Top Word Count')
plt %>% ggplotly()
save(word.count,file="word_count_wordcloud.rda")
#wordcloud
wordcloud(
words = word.count$word,
freq = word.count$n,
min.freq = 100,
colors = brewer.pal(8, 'Dark2')
)
#We can run an analogous analysis for hastags.
hashtags.unnested.df <- tweets.df %>%
select( Hashtags) %>%
unnest_tokens(input = Hashtags, output = hashtag)
hashtags.unnested.count <- hashtags.unnested.df %>%
count(hashtag) %>%
drop_na()
#We plot the correspondinng word cloud.
save(hashtags.unnested.count,file="hashtags_unnested_count.rda")
runApp()
runApp()
runApp()
runApp()
